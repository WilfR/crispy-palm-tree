{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VbXLWxvakGd"
   },
   "source": [
    "# Medical image analysis with PyTorch\n",
    "\n",
    "Create a deep convolutional network for an image translation task with PyTorch from scratch and train it on a subset of the IXI dataset for a T1-w to T2-w transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMEVHtsQakHg"
   },
   "source": [
    "## Step 4: Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(i,o):\n",
    "    return (nn.Conv3d(i,o,3,padding=1,bias=False), \n",
    "            nn.BatchNorm3d(o), \n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "def unet_block(i,m,o):\n",
    "    return nn.Sequential(*conv(i,m),*conv(m,o))\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, s=32):\n",
    "        super().__init__()\n",
    "        self.start = unet_block(1,s,s)\n",
    "        self.down1 = unet_block(s,s*2,s*2)\n",
    "        self.down2 = unet_block(s*2,s*4,s*4)\n",
    "        self.bridge = unet_block(s*4,s*8,s*4)\n",
    "        self.up2 = unet_block(s*8,s*4,s*2)\n",
    "        self.up1 = unet_block(s*4,s*2,s)\n",
    "        self.final = nn.Sequential(*conv(s*2,s),nn.Conv3d(s,1,1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        r = [self.start(x)]\n",
    "        r.append(self.down1(F.max_pool3d(r[-1],2)))\n",
    "        r.append(self.down2(F.max_pool3d(r[-1],2)))\n",
    "        x = F.interpolate(self.bridge(F.max_pool3d(r[-1],2)),size=r[-1].shape[2:])\n",
    "        x = F.interpolate(self.up2(torch.cat((x,r[-1]),dim=1)),size=r[-2].shape[2:])\n",
    "        x = F.interpolate(self.up1(torch.cat((x,r[-2]),dim=1)),size=r[-3].shape[2:])\n",
    "        x = self.final(torch.cat((x,r[-3]),dim=1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0q6IsTOoakHh"
   },
   "outputs": [],
   "source": [
    "valid_split = 0.1\n",
    "batch_size = 16\n",
    "n_jobs = 12\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcRgNZ-EakHi"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Compose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2c8546b997de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRandomCrop3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# set up training and validation data loader for nifti images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNiftiDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# set preload=False if you have limited CPU memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Compose' is not defined"
     ]
    }
   ],
   "source": [
    "tfms = Compose([RandomCrop3D((128,128,32)), ToTensor()])\n",
    "\n",
    "# set up training and validation data loader for nifti images\n",
    "dataset = NiftiDataset(t1_dir, t2_dir, tfms, preload=False)  # set preload=False if you have limited CPU memory\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(valid_split * num_train)\n",
    "valid_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(valid_idx))\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size,\n",
    "                          num_workers=n_jobs, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset, sampler=valid_sampler, batch_size=batch_size,\n",
    "                          num_workers=n_jobs, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2k7lcMnGakHj"
   },
   "source": [
    "### Milestone 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xTvuKC9akHk"
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bm9hqmn9akHm"
   },
   "outputs": [],
   "source": [
    "model = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwGhrHU2akHp"
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('trained.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K044-y4cakHq"
   },
   "outputs": [],
   "source": [
    "model.cuda(device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "criterion = nn.SmoothL1Loss()  #nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvcONTuwakHs",
    "outputId": "b161d800-a619-4301-b5be-e12c44d13271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Training Loss: 1.44e-01, Validation Loss: 8.80e-01\n",
      "Epoch: 2 - Training Loss: 1.20e-01, Validation Loss: 2.06e-01\n",
      "Epoch: 3 - Training Loss: 1.17e-01, Validation Loss: 1.63e-01\n",
      "Epoch: 4 - Training Loss: 1.09e-01, Validation Loss: 1.21e-01\n",
      "Epoch: 5 - Training Loss: 1.05e-01, Validation Loss: 1.01e-01\n",
      "Epoch: 6 - Training Loss: 1.01e-01, Validation Loss: 1.06e-01\n",
      "Epoch: 7 - Training Loss: 1.04e-01, Validation Loss: 3.73e-01\n",
      "Epoch: 8 - Training Loss: 1.02e-01, Validation Loss: 1.15e-01\n",
      "Epoch: 9 - Training Loss: 1.01e-01, Validation Loss: 1.97e-01\n",
      "Epoch: 10 - Training Loss: 9.05e-02, Validation Loss: 1.41e-01\n",
      "Epoch: 11 - Training Loss: 8.75e-02, Validation Loss: 9.73e-02\n",
      "Epoch: 12 - Training Loss: 9.25e-02, Validation Loss: 1.21e-01\n",
      "Epoch: 13 - Training Loss: 8.07e-02, Validation Loss: 1.14e-01\n",
      "Epoch: 14 - Training Loss: 9.38e-02, Validation Loss: 1.12e-01\n",
      "Epoch: 15 - Training Loss: 9.20e-02, Validation Loss: 8.68e-02\n",
      "Epoch: 16 - Training Loss: 8.71e-02, Validation Loss: 9.11e-02\n",
      "Epoch: 17 - Training Loss: 8.63e-02, Validation Loss: 8.77e-02\n",
      "Epoch: 18 - Training Loss: 8.17e-02, Validation Loss: 9.69e-02\n",
      "Epoch: 19 - Training Loss: 8.32e-02, Validation Loss: 8.04e-02\n",
      "Epoch: 20 - Training Loss: 8.91e-02, Validation Loss: 2.12e-01\n",
      "Epoch: 21 - Training Loss: 9.98e-02, Validation Loss: 7.94e-02\n",
      "Epoch: 22 - Training Loss: 8.58e-02, Validation Loss: 9.54e-02\n",
      "Epoch: 23 - Training Loss: 8.66e-02, Validation Loss: 8.81e-02\n",
      "Epoch: 24 - Training Loss: 8.22e-02, Validation Loss: 1.77e-01\n",
      "Epoch: 25 - Training Loss: 7.61e-02, Validation Loss: 7.83e-02\n",
      "Epoch: 26 - Training Loss: 7.91e-02, Validation Loss: 1.23e-01\n",
      "Epoch: 27 - Training Loss: 7.53e-02, Validation Loss: 7.28e-02\n",
      "Epoch: 28 - Training Loss: 7.57e-02, Validation Loss: 6.92e-02\n",
      "Epoch: 29 - Training Loss: 8.08e-02, Validation Loss: 8.17e-02\n",
      "Epoch: 30 - Training Loss: 7.74e-02, Validation Loss: 8.11e-02\n",
      "Epoch: 31 - Training Loss: 8.36e-02, Validation Loss: 8.37e-02\n",
      "Epoch: 32 - Training Loss: 8.11e-02, Validation Loss: 8.54e-02\n",
      "Epoch: 33 - Training Loss: 7.36e-02, Validation Loss: 6.93e-02\n",
      "Epoch: 34 - Training Loss: 7.37e-02, Validation Loss: 8.33e-02\n",
      "Epoch: 35 - Training Loss: 7.40e-02, Validation Loss: 8.86e-02\n",
      "Epoch: 36 - Training Loss: 7.45e-02, Validation Loss: 7.63e-02\n",
      "Epoch: 37 - Training Loss: 8.08e-02, Validation Loss: 1.75e-01\n",
      "Epoch: 38 - Training Loss: 7.84e-02, Validation Loss: 8.48e-02\n",
      "Epoch: 39 - Training Loss: 7.70e-02, Validation Loss: 8.39e-02\n",
      "Epoch: 40 - Training Loss: 7.06e-02, Validation Loss: 8.10e-02\n",
      "Epoch: 41 - Training Loss: 7.31e-02, Validation Loss: 1.04e-01\n",
      "Epoch: 42 - Training Loss: 7.48e-02, Validation Loss: 8.68e-02\n",
      "Epoch: 43 - Training Loss: 6.78e-02, Validation Loss: 2.42e-01\n",
      "Epoch: 44 - Training Loss: 6.52e-02, Validation Loss: 1.16e-01\n",
      "Epoch: 45 - Training Loss: 6.64e-02, Validation Loss: 1.09e-01\n",
      "Epoch: 46 - Training Loss: 6.59e-02, Validation Loss: 9.96e-02\n",
      "Epoch: 47 - Training Loss: 6.75e-02, Validation Loss: 6.96e-02\n",
      "Epoch: 48 - Training Loss: 6.69e-02, Validation Loss: 9.90e-02\n",
      "Epoch: 49 - Training Loss: 6.99e-02, Validation Loss: 7.00e-02\n",
      "Epoch: 50 - Training Loss: 7.23e-02, Validation Loss: 7.79e-02\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "n_batches = len(train_loader)\n",
    "for t in range(1, n_epochs + 1):\n",
    "    # training\n",
    "    t_losses = []\n",
    "    model.train(True)\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src)\n",
    "        loss = criterion(out, tgt)\n",
    "        t_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(t_losses)\n",
    "\n",
    "    # validation\n",
    "    v_losses = []\n",
    "    model.train(False)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for src, tgt in valid_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            out = model(src)\n",
    "            loss = criterion(out, tgt)\n",
    "            v_losses.append(loss.item())\n",
    "        valid_losses.append(v_losses)\n",
    "\n",
    "    if not np.all(np.isfinite(t_losses)): \n",
    "        raise RuntimeError('NaN or Inf in training loss, cannot recover. Exiting.')\n",
    "    log = f'Epoch: {t} - Training Loss: {np.mean(t_losses):.2e}, Validation Loss: {np.mean(v_losses):.2e}'\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEgRD9n8akHu"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. Train the network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
